{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDib219Y1e4z"
      },
      "source": [
        "# TL;DR\n",
        "\n",
        "In this lab scenario you will finish implementation of a variant of the Q-learning method, called DQN. On top of the usual q-learning using neural nets as function approximations, DQN uses:\n",
        "* experience replay - used to increase efficacy of samples from the environment and decorrelate elements of a batch,\n",
        "* target network - used to avoid constantly changing targets in the learning process (to avoid \"chasing own tail\").\n",
        "\n",
        "For algorithm's details recall the lecture and/or follow the [original paper](https://arxiv.org/abs/1312.5602), which is rather self-contained and not hard to understand.\n",
        "\n",
        "Without changing any hyperparameters, the agent should solve the problem (obtain rewards ~200) after ~1000 episodes, which for GPU runtime takes ~10 minutes of training.\n",
        "\n",
        "You can run this code locally (not in Colab), which allows to see the agent in action, unfortunately visualization inside Colab worked poorly and was removed from this lab scenario."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbWvFo-wAP_N"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07GoFJCXC3li",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77311783-6693-4720-d6c9-d26235b0bf37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Box2D==2.3.10 in /usr/local/lib/python3.11/dist-packages (2.3.10)\n",
            "Requirement already satisfied: gym==0.26.2 in /usr/local/lib/python3.11/dist-packages (0.26.2)\n",
            "Requirement already satisfied: pygame==2.6.1 in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from gym==0.26.2) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym==0.26.2) (3.1.1)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym==0.26.2) (0.0.8)\n",
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.70.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.7)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboard) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (4.25.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (75.1.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.55.7)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (7.7.1)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (5.5.6)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (3.6.10)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (3.0.13)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (75.1.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.5.5)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.5)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (24.0.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (23.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.7.2)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (7.16.6)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.6.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.21.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.13)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.3.6)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.12.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.1)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.10.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (24.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.23.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.1->jupyter-client->ipykernel>=4.5.1->ipywidgets) (1.17.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.22.3)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.11/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.24.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.22)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.0)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from referencing>=0.28.4->jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (1.26.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install Box2D==2.3.10 gym==0.26.2 pygame==2.6.1\n",
        "!pip install numpy==1.26.4 torch\n",
        "!pip install tensorboard\n",
        "!pip install matplotlib\n",
        "!pip install ipywidgets\n",
        "!pip install opencv-python\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Le1nHKNN9_xa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aef85307-9912-437a-9532-d9b4489e3977"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import numpy as np\n",
        "import argparse\n",
        "import datetime\n",
        "import time\n",
        "import random\n",
        "from collections import namedtuple\n",
        "from pathlib import Path\n",
        "from typing import Tuple, List\n",
        "from matplotlib import pyplot as plt\n",
        "import IPython.display as display\n",
        "import ipywidgets as widgets\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Used to speed up the training in colab and avoid situations when the model is stuck\n",
        "# But this can be problematic for our model (why?)\n",
        "MAX_EPISODE_STEPS = 500\n",
        "\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quBickk6-Gxb"
      },
      "source": [
        "# Utilities\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2Y7aDTODOXq"
      },
      "source": [
        "## Misc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ptz2QtxJDPXw"
      },
      "outputs": [],
      "source": [
        "def try_gpu(i: int = 0):\n",
        "    \"\"\"Return gpu(i) if exists, otherwise return cpu()\"\"\"\n",
        "    if torch.cuda.device_count() >= i + 1:\n",
        "        return torch.device(f\"cuda:{i}\")\n",
        "    return torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "def save_model(model: nn.Module, PATH: str):\n",
        "    \"\"\"Saves model's state_dict.\n",
        "\n",
        "    Reference: https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
        "    \"\"\"\n",
        "    torch.save(model.state_dict(), PATH)\n",
        "\n",
        "\n",
        "def load_model(model: nn.Module, PATH: str):\n",
        "    \"\"\"Loads model's parameters from state_dict\"\"\"\n",
        "    model.load_state_dict(torch.load(PATH))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWGKDXkY-U9C"
      },
      "source": [
        "## Scheduler\n",
        "\n",
        "Training RL agents requires dealing with exploration-exploitation trade-off. To handle this we will adopt the most basic, but extremely efficient, epsilon-greedy strategy. At the beginning our agent will focus on exploration, and over time will start exploiting his knowledge, and thus becoming more and more greedy. To implement this logic we will use LinearDecay scheduler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zl5GIWFJ-bsp"
      },
      "outputs": [],
      "source": [
        "class Constant:\n",
        "    \"\"\"Constant scheduler.\n",
        "\n",
        "    Can be used e.g. to create agent with with greedy policy, namely epsilon == 0\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, value: float, *args):\n",
        "        self._value = value\n",
        "\n",
        "    def value(self, *args):\n",
        "        return self._value\n",
        "\n",
        "\n",
        "class LinearDecay:\n",
        "    \"\"\"Linear decay scheduler.\n",
        "\n",
        "    At each call linearly decays the value by simply subtracting `decay` from the current value,\n",
        "    until some minimum value is reached.\n",
        "    Can be used e.g. to decay epsilon value for epsilon-greedy exploration/exploitation strategy.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, initial_value: float, final_value: float, decay: float):\n",
        "        self._value = initial_value\n",
        "        self.final_value = final_value\n",
        "        self.decay = decay\n",
        "\n",
        "    def value(self, *args) -> float:\n",
        "        self._value = max(self.final_value, self._value - self.decay)\n",
        "        return self._value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPds8uarDZUE"
      },
      "source": [
        "## Replay buffer\n",
        "\n",
        "The key trick that makes DQN feasible is replay buffer. The idea is to store observed transitions, sample them randomly and perform updates based on them. This solution has many advantages, the most significant ones are:\n",
        "\n",
        "1.   *Data efficiency* - each transition (env step) can be used in many weight updates.\n",
        "2.   *Data decorrelation* - consecutive transitions are naturally highly correlated. Randomizing the samples reduces these correlations, thus reducing variance of the updates.\n",
        "\n",
        "Note that when learning by experience replay, it is necessary to learn off-policy (because our current parameters are different to those used to generate the sample), which motivates the choice of Q-learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oI62-4PtDb2v"
      },
      "outputs": [],
      "source": [
        "# non_terminal_mask is a mask indicating whether the state is terminal or not\n",
        "# it will become usefull when using target_net for predicting qvalues.\n",
        "Transition = namedtuple(\n",
        "    \"Transition\", (\"state\", \"action\", \"next_state\", \"reward\", \"non_terminal_mask\")\n",
        ")\n",
        "\n",
        "\n",
        "class ReplayBuffer(object):\n",
        "    def __init__(self, size: int):\n",
        "        \"\"\"Create new replay buffer.\n",
        "\n",
        "        Args:\n",
        "            size: capacity of the buffer\n",
        "        \"\"\"\n",
        "        self._storage: List[Transition] = []\n",
        "        self._capacity = size\n",
        "        self._next_idx = 0\n",
        "\n",
        "    def add(self, data: Transition):\n",
        "        if len(self._storage) < self._capacity:\n",
        "            self._storage.append(None)\n",
        "        self._storage[self._next_idx] = data\n",
        "        self._next_idx = (self._next_idx + 1) % self._capacity\n",
        "\n",
        "    def sample(self, batch_size: int) -> List[Transition]:\n",
        "        \"\"\"Sample batch of eixperience from memory.\n",
        "\n",
        "        Args:\n",
        "            batch_size: size of the batch\n",
        "\n",
        "        Returns:\n",
        "            batch of transitions\n",
        "        \"\"\"\n",
        "        batch = random.sample(self._storage, batch_size)\n",
        "        return batch\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self._storage)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCQIj0ycAim0"
      },
      "source": [
        "## MLP Network\n",
        "\n",
        "For fast iteration we will stick to numerical observations (original DQN paper works with graphical observations). We will use simple MLP to net approximate our estimates of Q-values for (action, states)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBkWJjn-AglG"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    \"\"\"Simple MLP net.\n",
        "\n",
        "    Each of the layers, despite the last one, is followed by ReLU non-linearity.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, layers_sizes: List[int]):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        modules = []\n",
        "        for in_features, out_features in zip(layers_sizes, layers_sizes[1:-1]):\n",
        "            modules.extend(\n",
        "                [\n",
        "                    nn.Linear(in_features, out_features),\n",
        "                    nn.ReLU(),\n",
        "                ]\n",
        "            )\n",
        "        # final output is not followed by non-linearity\n",
        "        modules.extend([nn.Linear(layers_sizes[-2], layers_sizes[-1])])\n",
        "        self.layers = nn.Sequential(*modules)\n",
        "\n",
        "    def forward(self, state):\n",
        "        return self.layers(state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKqQYX7HBBOw"
      },
      "source": [
        "# DQN Agent\n",
        "\n",
        "First we implement constructor and some utility functions for the agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6C4yaDjB8FnV"
      },
      "outputs": [],
      "source": [
        "class DQNAgent:\n",
        "    def __init__(\n",
        "        self, exploration_fn, policy_net: torch.nn.Module, target_net: torch.nn.Module\n",
        "    ):\n",
        "        self.exploration_fn = exploration_fn\n",
        "        self.policy_net = policy_net\n",
        "        self.target_net = target_net\n",
        "        self.optim = None\n",
        "        self.replay_buffer = None\n",
        "\n",
        "    def save_policy_net(self, checkpoint: str):\n",
        "        \"\"\"Saves policy_net parameters as given checkpoint.\n",
        "\n",
        "        state_dict of current policy_net is stored.\n",
        "\n",
        "        Args:\n",
        "            checkpoint: path were to store model's parameters.\n",
        "        \"\"\"\n",
        "        save_model(self.policy_net, checkpoint)\n",
        "\n",
        "    def load_policy_net(self, checkpoint: str):\n",
        "        \"\"\"Loads policy_net parameters from given checkpoint.\n",
        "\n",
        "        Note that proper model should be instantiated as only parameters of form state_dict\n",
        "        are stored as a checkpoint.\n",
        "\n",
        "        Args:\n",
        "            checkpoint: path to model's parameters.\n",
        "        \"\"\"\n",
        "        load_model(self.policy_net, checkpoint)\n",
        "\n",
        "    def play_episodes(self, n_episodes: int, env: gym.Env):\n",
        "        \"\"\"Function to watch the agent playing - locally\"\"\"\n",
        "\n",
        "        def render(frame, widget):\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "            _, jpeg = cv2.imencode(\".jpeg\", frame)\n",
        "            widget.value = jpeg.tobytes()\n",
        "\n",
        "        self.policy_net.eval()\n",
        "\n",
        "        for episode in range(n_episodes):\n",
        "            # 0.5 sec breaks between episodes, so it's easier to watch\n",
        "            time.sleep(0.5)\n",
        "            state = env.reset()\n",
        "\n",
        "            if isinstance(state, tuple):\n",
        "                state = state[0]\n",
        "\n",
        "            total_reward, timesteps, done = 0, 0, False\n",
        "            image_widget = widgets.Image(format=\"jpeg\")\n",
        "            display.display(image_widget)\n",
        "            frame = env.render()\n",
        "            render(frame=frame, widget=image_widget)\n",
        "\n",
        "            episode_steps_left = MAX_EPISODE_STEPS\n",
        "            while not done:\n",
        "                episode_steps_left -= 1\n",
        "                # Pick next action, simulate and observe next_state and reward\n",
        "                action = self.act(state)\n",
        "                next_state, reward, done, _, _ = env.step(action.item())\n",
        "                done = done or episode_steps_left <= 0\n",
        "                state = next_state\n",
        "\n",
        "                frame = env.render()\n",
        "                render(frame=frame, widget=image_widget)\n",
        "                # To make watching easier\n",
        "                time.sleep(0.01)\n",
        "\n",
        "                total_reward += reward\n",
        "                timesteps += 1\n",
        "\n",
        "            print(f\"Episode length: {timesteps}, total reward: {total_reward}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pK0xexE68O7x"
      },
      "source": [
        "### Policy\n",
        "\n",
        "Given observation agent follows epsilon-greedy strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bs3kBCu68Mrk"
      },
      "outputs": [],
      "source": [
        "class DQNAgent(DQNAgent):\n",
        "    def act(self, obs) -> torch.Tensor:\n",
        "        \"\"\"Epsilon-greedy policy derived from policy_net\n",
        "\n",
        "        With probability epsilon select a random action a_t.\n",
        "        Otherwise select a_t = max_a(Q(obs, a; theta))\n",
        "        \"\"\"\n",
        "        eps_exploration = self.exploration_fn.value()\n",
        "        if torch.rand(1).item() <= eps_exploration:\n",
        "            return torch.randint(0, N_ACTIONS, [1])\n",
        "        else:\n",
        "            if not type(obs) == torch.Tensor:\n",
        "                obs = torch.tensor(obs, dtype=torch.float32, device=DEVICE).view(\n",
        "                    -1, OBS_SHAPE\n",
        "                )\n",
        "            with torch.no_grad():\n",
        "                return torch.argmax(self.policy_net(obs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wue8RcczBsaB"
      },
      "source": [
        "### Learning procedure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRVzkxNLBmQ3"
      },
      "outputs": [],
      "source": [
        "class DQNAgent(DQNAgent):\n",
        "    def learn(\n",
        "        self,\n",
        "        gamma: float,\n",
        "        optim: torch.optim.Optimizer,\n",
        "        n_episodes: int,\n",
        "        batch_size: int,\n",
        "        target_update_interval: int,\n",
        "        buffer_size: int,\n",
        "        checkpoints_dir: str,\n",
        "        checkpoint_save_interval: int,\n",
        "        tensorboard_log_dir: str,\n",
        "        env: gym.Env,\n",
        "    ):\n",
        "        self.optim = optim\n",
        "        self.replay_buffer = ReplayBuffer(buffer_size)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.target_net.eval()\n",
        "        self.policy_net.train()\n",
        "\n",
        "        total_steps, rewards_history = 0, []\n",
        "        writer = SummaryWriter(tensorboard_log_dir)\n",
        "\n",
        "        for episode in tqdm(range(n_episodes), desc=\"Training episode\"):\n",
        "            episode_reward, episode_steps, done = 0, 0, False\n",
        "            state = env.reset()\n",
        "\n",
        "            if isinstance(state, tuple):\n",
        "                state = state[0]\n",
        "            episode_steps_left = MAX_EPISODE_STEPS\n",
        "            while not done:\n",
        "                episode_steps_left -= 1\n",
        "                # Pick next action, simulate and observe next_state and reward\n",
        "                action = self.act(state)\n",
        "                next_state, reward, done, _, _ = env.step(action.item())\n",
        "                done = done or episode_steps_left <= 0\n",
        "\n",
        "                ##### TODO IMPLEMENT #####\n",
        "                # Store Transition in replay buffer. (\"state\", \"action\", \"next_state\", \"reward\", \"non_terminal_mask\")\n",
        "                self.replay_buffer.add(\n",
        "                    data=Transition(\n",
        "                        state=state,\n",
        "                        action=action.item(),\n",
        "                        next_state=next_state,\n",
        "                        reward=reward,\n",
        "                        non_terminal_mask=0 if done else 1,\n",
        "                    )\n",
        "                )\n",
        "                ##### END OF TODO    #####\n",
        "\n",
        "                # Update target_net\n",
        "                loss = self._update_policy_net(gamma, batch_size)\n",
        "\n",
        "                # Update current state\n",
        "                state = next_state\n",
        "\n",
        "                # Update target_net with current parameters\n",
        "                if (total_steps + 1) % target_update_interval == 0:\n",
        "                    self._update_target_net()\n",
        "                #\n",
        "                if (total_steps + 1) % checkpoint_save_interval == 0:\n",
        "                    self.save_policy_net(\n",
        "                        f\"{checkpoints_dir}/params_nsteps{total_steps + 1}_nepis{episode}\"\n",
        "                    )\n",
        "\n",
        "                # Misc\n",
        "                total_steps += 1\n",
        "                episode_steps += 1\n",
        "                episode_reward += reward\n",
        "                if loss:\n",
        "                    writer.add_scalar(\"Loss/MSE\", loss, total_steps)\n",
        "\n",
        "            rewards_history.append(episode_reward)\n",
        "            # Tensorboard\n",
        "            writer.add_scalar(\"Reward/episode\", episode_reward, episode)\n",
        "            writer.add_scalar(\n",
        "                \"Reward/mean_100_episodes\", np.mean(rewards_history[-100:]), episode\n",
        "            )\n",
        "            writer.add_scalar(\"Episode/n_steps\", episode_steps, episode)\n",
        "            writer.add_scalar(\n",
        "                \"Misc/eps_exploration\", self.exploration_fn._value, episode\n",
        "            )\n",
        "\n",
        "        writer.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7cminMw81ko"
      },
      "source": [
        "### PolicyNet update step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEzYbq-hPpLb"
      },
      "outputs": [],
      "source": [
        "class DQNAgent(DQNAgent):\n",
        "    def _update_policy_net(self, gamma: float, batch_size: int):\n",
        "        \"\"\"Perform one round of policy_net update.\n",
        "\n",
        "        Sample random minibatch of transitions (fi(s_t), a_t, r_t, fi(s_t+1)) from replay buffer\n",
        "        and update policy_net according to DQN algorithm.\n",
        "        \"\"\"\n",
        "        if len(self.replay_buffer) < batch_size:\n",
        "            return\n",
        "\n",
        "        def get_targets(gamma: float, batch: Transition):\n",
        "            \"\"\"Uses `target_net` and immediate rewards to calculate expected future rewards.\"\"\"\n",
        "            batch_next_state = torch.tensor(batch.next_state, device=DEVICE).detach()\n",
        "            # target_net prediction for terminal states should be 0, as our expectation from terminal state is 0\n",
        "            non_terminal_mask = torch.tensor(\n",
        "                batch.non_terminal_mask, device=DEVICE\n",
        "            ).detach()\n",
        "            next_state_bootstrapped_values = (\n",
        "                torch.max(self.target_net(batch_next_state), dim=1)[0].detach()\n",
        "                * non_terminal_mask\n",
        "            )\n",
        "            assert torch.all(\n",
        "                (non_terminal_mask == 0).nonzero()\n",
        "                == (next_state_bootstrapped_values == 0).nonzero()\n",
        "            )\n",
        "\n",
        "            assert len(batch.reward.shape) == 1\n",
        "            assert len(next_state_bootstrapped_values.shape) == 1\n",
        "            ##### TODO IMPLEMENT - given the pieces from above, compute the targets #####\n",
        "            # to match remaining portions of the code, reshape the target tensor as follows: (-1, 1)\n",
        "            targets = (\n",
        "                torch.tensor(batch.reward, device=DEVICE)\n",
        "                + gamma * next_state_bootstrapped_values\n",
        "            )\n",
        "            # Expected future reward for terminal state is equal to immediate reward\n",
        "            # For non terminal states expected future reward:\n",
        "            # immediate reward + discounted future expectation\n",
        "            ##### END OF TODO    #####\n",
        "\n",
        "            assert targets.shape == (batch.next_state.shape[0], 1)\n",
        "            return targets\n",
        "\n",
        "        def get_state_action_values(batch):\n",
        "            \"\"\"Uses `policy_net` to calculate current estimates of future rewards.\"\"\"\n",
        "            batch_state = torch.tensor(batch.state, device=DEVICE)\n",
        "            # Calculate current estimates for the (state, action) we have observed and taken\n",
        "            # 'preds' shape: (batch_size, n_states, n_actions)\n",
        "            preds = self.policy_net(batch_state)\n",
        "            # Extracting values from various indices might be a little confusing:\n",
        "            # https://medium.com/analytics-vidhya/understanding-indexing-with-pytorch-gather-33717a84ebc4\n",
        "            action_index = torch.tensor(\n",
        "                batch.action, dtype=torch.long, device=DEVICE\n",
        "            ).unsqueeze(-1)\n",
        "            state_action_values = torch.gather(preds, dim=1, index=action_index)\n",
        "            return state_action_values\n",
        "\n",
        "        # Sample and convert batch into big Transition of form:\n",
        "        # Transition(state=(0,0,...), action=(1,4,...), next_state=(0,3,...), reward(3,0,...), non_terminal_mask(0,1,0,...))\n",
        "        # In other words: list_of_tuples -> tuple_of_lists\n",
        "        transitions = self.replay_buffer.sample(batch_size)\n",
        "        batch = Transition(*zip(*transitions))\n",
        "        # Convert to numpy arrays so that we can use binary mask as indices to extract e.g. non terminal masks\n",
        "        # Types are chosen so that torch.tensor will inherit correct one\n",
        "        batch = Transition(\n",
        "            np.array(batch.state),\n",
        "            np.array(batch.action),\n",
        "            np.array(batch.next_state),\n",
        "            np.array(batch.reward, np.float32),\n",
        "            np.array(batch.non_terminal_mask, np.float32),\n",
        "        )\n",
        "\n",
        "        state_action_values = get_state_action_values(batch)\n",
        "        targets = get_targets(gamma, batch)\n",
        "        loss = F.mse_loss(state_action_values, targets)\n",
        "        self.optim.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optim.step()\n",
        "\n",
        "        return loss.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3I1pWY5PzBa"
      },
      "source": [
        "### TargetNet update\n",
        "Finally, the last missing step is to *update target_net*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3JSgqcqPvCM"
      },
      "outputs": [],
      "source": [
        "class DQNAgent(DQNAgent):\n",
        "    def _update_target_net(self):\n",
        "        \"\"\"Sets `target_net` parameters to the current `policy_net` parameters.\"\"\"\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NM2rsPcFoQZ"
      },
      "source": [
        "# Environment\n",
        "\n",
        "We will try to solve: https://gym.openai.com/envs/LunarLander-v2/\n",
        "\n",
        "LunearLander env can be considered solved once we achieve 200 points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qxq_h-VFn-t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb9a20e8-bcda-448f-f065-098cc1000e2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of actions = 4\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")\n",
        "N_ACTIONS = env.action_space.n\n",
        "OBS_SHAPE = 8\n",
        "print(f\"Number of actions = {N_ACTIONS}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCDEETSwZP4I"
      },
      "source": [
        "# Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3TKjP_iGPgV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7df7b73b-7dfa-41d0-cf2f-14fe87221304"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "DEVICE = try_gpu()\n",
        "print(DEVICE)\n",
        "\n",
        "EXP_NAME = \"LunarLander\"\n",
        "LOG_DIR = f\"runs/{EXP_NAME}\"\n",
        "TENSORBOARD_LOG_DIR = f\"runs/{EXP_NAME}/tensorboard\"\n",
        "CHECKPOINTS_DIR = f\"runs/{EXP_NAME}/checkpoints\"\n",
        "Path(CHECKPOINTS_DIR).mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7DI30GqKAQC"
      },
      "outputs": [],
      "source": [
        "def parse_args():\n",
        "    # Training settings\n",
        "    parser = argparse.ArgumentParser(description=\"PyTorch DQN implementation\")\n",
        "\n",
        "    # Hack for colab...\n",
        "    parser.add_argument(\n",
        "        \"-f\",\n",
        "        \"--fff\",\n",
        "        help=\"a dummy argument to fool ipython in colab. Comment out for local dev.\",\n",
        "        default=\"1\",\n",
        "    )\n",
        "\n",
        "    # To see the agent playing\n",
        "    parser.add_argument(\n",
        "        \"--play\",\n",
        "        type=bool,\n",
        "        default=False,\n",
        "        help=\"play mode, if True then agent will play env instead of do training (default: False). \"\n",
        "        \"If checkpoint is not specified then randomly initialized network will play\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--checkpoint\",\n",
        "        type=str,\n",
        "        default=None,\n",
        "        help=\"checkpoint storing state_dict to load for the model. \"\n",
        "        \"If None then agent will be initialized with random params (default: None)\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--n_episodes\", type=int, default=10, help=\"number of episodes to play\"\n",
        "    )\n",
        "\n",
        "    # To train the agent\n",
        "    parser.add_argument(\n",
        "        \"--exp_dir\",\n",
        "        type=str,\n",
        "        default=f\"exp/{datetime.datetime.now().timestamp()}\",\n",
        "        help=\"experiment directory were logs and checkpoints will be stored (default: exp/{timestamp}\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--batch-size\",\n",
        "        type=int,\n",
        "        default=64,\n",
        "        metavar=\"N\",\n",
        "        help=\"input batch size for training (default: 64)\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--epochs\",\n",
        "        type=int,\n",
        "        default=14,\n",
        "        metavar=\"N\",\n",
        "        help=\"number of epochs to train (default: 5000)\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--lr\",\n",
        "        type=float,\n",
        "        default=0.0005,\n",
        "        metavar=\"LR\",\n",
        "        help=\"learning rate (default: 0.0005)\",\n",
        "    )\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    return args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9twCGQ59vnf"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    args = parse_args()\n",
        "\n",
        "    layers = [OBS_SHAPE, 256, 256, N_ACTIONS]\n",
        "    policy_net = MLP(layers).to(DEVICE)\n",
        "    target_net = MLP(layers).to(DEVICE)\n",
        "\n",
        "    agent_params = {\n",
        "        \"exploration_fn\": LinearDecay(1, 0.05, 0.00001),\n",
        "        \"policy_net\": policy_net,\n",
        "        \"target_net\": target_net,\n",
        "    }\n",
        "    if args.play:\n",
        "        print(\"Wanna play a game...\")\n",
        "        agent_params[\"exploration_fn\"] = Constant(0.01)\n",
        "        agent = DQNAgent(**agent_params)\n",
        "        if args.checkpoint:\n",
        "            agent.load_policy_net(args.checkpoint)\n",
        "        agent.play_episodes(args.n_episodes, env=env)\n",
        "\n",
        "    else:\n",
        "        print(\"Training mode...\")\n",
        "        train_params = {\n",
        "            \"gamma\": 0.99,\n",
        "            \"optim\": torch.optim.Adam(policy_net.parameters(), lr=0.0005),\n",
        "            \"n_episodes\": int(2e4),\n",
        "            \"batch_size\": 64,\n",
        "            # Target update interval in number of env steps (not episodes)\n",
        "            \"target_update_interval\": 100,\n",
        "            \"buffer_size\": 10000,\n",
        "            \"checkpoint_save_interval\": 5000,\n",
        "            \"checkpoints_dir\": CHECKPOINTS_DIR,\n",
        "            \"tensorboard_log_dir\": TENSORBOARD_LOG_DIR,\n",
        "        }\n",
        "\n",
        "        agent = DQNAgent(**agent_params)\n",
        "        agent.learn(**train_params, env=env)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Fcy_lr_lT4P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 838
        },
        "outputId": "9353caef-f501-46b9-a92d-465abd4ad2f0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 11680), started 0:00:21 ago. (Use '!kill 11680' to kill it.)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        (async () => {\n",
              "            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n",
              "            url.searchParams.set('tensorboardColab', 'true');\n",
              "            const iframe = document.createElement('iframe');\n",
              "            iframe.src = url;\n",
              "            iframe.setAttribute('width', '100%');\n",
              "            iframe.setAttribute('height', '800');\n",
              "            iframe.setAttribute('frameborder', 0);\n",
              "            document.body.appendChild(iframe);\n",
              "        })();\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Start tensorboard in google colab\n",
        "# If you can't see anything run this cell twice\n",
        "%tensorboard --logdir $TENSORBOARD_LOG_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwNTgAuKWEcj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "outputId": "9911ea70-803e-4a0b-f2db-2b073841f697"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training mode...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining episode:   0%|          | 0/20000 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n",
            "Training episode:   0%|          | 0/20000 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-263240bbee7e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-1828b1304798>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQNAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0magent_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-14c5303b1c2d>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, gamma, optim, n_episodes, batch_size, target_update_interval, buffer_size, checkpoints_dir, checkpoint_save_interval, tensorboard_log_dir, env)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0;31m# Update target_net\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_policy_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0;31m# Update current state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-222226c378d6>\u001b[0m in \u001b[0;36m_update_policy_net\u001b[0;34m(self, gamma, batch_size)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mstate_action_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_state_action_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_action_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-222226c378d6>\u001b[0m in \u001b[0;36mget_targets\u001b[0;34m(gamma, batch)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m##### END OF TODO    #####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUejGPxP5Bda"
      },
      "source": [
        "# Tasks\n",
        "\n",
        "\n",
        "1.   Implement missing code #### TODO IMPLEMENT #####\n",
        "2.   Experiment with the hyperparameters e.g. gamma (discount-factor), epsilon (for exploration-exploitation trade-off)\n",
        "3.   Observe weird behaviors of agent, e.g. \"forgetting how to play\" - reward going significantly down, and then \"re-learning\" again. Why can it happen? What can we do to avoid it?\n",
        "4.   Change the args and observe the trained model behavior. What do you see?\n",
        "5.   What can be improved in the training code?\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}